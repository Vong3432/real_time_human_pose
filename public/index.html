<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real time human pose</title>
</head>

<body>
    <video width="600" height="480" autoplay style="display: none;" muted id="video"></video>
    <canvas style="float:right" width="600" height="480" id="canvas"></canvas>

    <a href="https://ml5js.org/">References from ML5 js</a>

    <!-- Load TensorFlow.js -->
    <!-- <script src="https://unpkg.com/@tensorflow/tfjs"></script> -->
    <!-- Load Posenet -->
    <!-- <script src="https://unpkg.com/@tensorflow-models/posenet"></script> -->    
    
    <!-- ml5 -->
    <script src="https://unpkg.com/ml5@latest/dist/ml5.min.js"></script>
    <script src="./js/script.js"></script>
    <script>
        const options = {
            imageScaleFactor: 0.5,
            flipHorizontal: true,
            outputStride: 16,
            detectionType: 'single',
        }

        let video = document.getElementById('video');
        let canvas = document.getElementById('canvas');
        let ctx = canvas.getContext("2d");  
        ctx.strokeStyle = "yellow";              
        ctx.lineWidth = 3;

        // The detected positions will be inside an array
        let poses = [];

        // Create a webcam capture
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
            navigator.mediaDevices.getUserMedia({
                video: true
            }).then(function (stream) {
                video.srcObject = stream;
                // video.play();
            });
        }

        // A function to draw the video and poses into the canvas.
        // This function is independent of the result of posenet
        // This way the video will not seem slow if poseNet
        // is not detecting a position
        function drawCameraIntoCanvas() {
            // Draw the video element into the canvas
            ctx.drawImage(video, 0, 0, 600, 480);
            // We can call both functions to draw all keypoints and the skeletons
            drawKeypoints();
            drawSkeleton();
            window.requestAnimationFrame(drawCameraIntoCanvas);
        }
        // Loop over the drawCameraIntoCanvas function
        drawCameraIntoCanvas();

        // Create a new poseNet method with a single detection
        const poseNet = ml5.poseNet(video, modelReady);
        poseNet.on("pose", gotPoses);

        // A function that gets called every time there's an update from the model
        function gotPoses(results) {
            poses = results;
        }

        function modelReady() {
            console.log("model ready");
            poseNet.multiPose(video);
        }

        // A function to draw ellipses over the detected keypoints
        function drawKeypoints() {
            // Loop through all the poses detected
            for (let i = 0; i < poses.length; i += 1) {
                // For each pose detected, loop through all the keypoints
                for (let j = 0; j < poses[i].pose.keypoints.length; j += 1) {
                    let keypoint = poses[i].pose.keypoints[j];
                    // Only draw an ellipse is the pose probability is bigger than 0.2
                    if (keypoint.score > 0.2) {
                        ctx.beginPath();                        
                        ctx.arc(keypoint.position.x, keypoint.position.y, 10, 0, 2 * Math.PI);
                        ctx.stroke();
                    }
                }
            }
        }

        // A function to draw the skeletons
        function drawSkeleton() {
            // Loop through all the skeletons detected
            for (let i = 0; i < poses.length; i += 1) {
                // For every skeleton, loop through all body connections
                for (let j = 0; j < poses[i].skeleton.length; j += 1) {
                    let partA = poses[i].skeleton[j][0];
                    let partB = poses[i].skeleton[j][1];                
                    ctx.beginPath();                    
                    ctx.moveTo(partA.position.x, partA.position.y);
                    ctx.lineTo(partB.position.x, partB.position.y);                    
                    ctx.stroke();
                }
            }
        }
    </script>
    <!-- <script type="text/javascript">               
        
        async function estimate(net) {
            const scaleFactor = 0.50;
            const flipHorizontal = true; // true for video
            const outputStride = 16;
            const video = document.getElementById('video');            

            return await net.estimateSinglePose(video, scaleFactor, flipHorizontal, outputStride);
        }

        posenet.load().then(async function (net) {            
            // setInterval(async function() {
            //     // load the posenet model        
            //     const result = await estimate(net);
            //     console.log(result);
            // }, 100)            

            const result = await estimate(net);
            console.log(result);
        });
    </script> -->
</body>

</html>